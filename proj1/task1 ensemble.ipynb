{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#IO\n",
    "trainXdf=pd.read_csv('X_train.csv')\n",
    "trainYdf=pd.read_csv('y_train.csv')\n",
    "testXdf=pd.read_csv('X_test.csv')\n",
    "trainX=trainXdf[[i for i in trainXdf.columns if i not in ['id']]]\n",
    "trainY=trainYdf[[i for i in trainYdf.columns if i not in ['id']]]\n",
    "#i will preprocess the columns\n",
    "X=trainX\n",
    "y=trainY['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import autosklearn.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from scipy.stats import pearsonr\n",
    "features = trainX.columns.tolist()\n",
    "target =  trainY.columns[0] #'y'\n",
    "\n",
    "pearsonSelectThreshold=0.2\n",
    "\n",
    "correlations = {}\n",
    "for f in features:\n",
    "    nonEmptyRows=~trainX[f].isnull()\n",
    "    x1 = trainX[f].values[nonEmptyRows]\n",
    "    x2 = trainY[target].values[nonEmptyRows]\n",
    "    correlations[f] = pearsonr(x1,x2)[0]\n",
    "    \n",
    "data_correlations = pd.DataFrame(correlations, index=['Value']).T\n",
    "featureImpotance=data_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]\n",
    "#filter columns that are irrelevant, such as contant columns, zero columns that may crash everything\n",
    "gdFeature=featureImpotance[np.abs(featureImpotance.Value)>pearsonSelectThreshold]\n",
    "print('features with higher correlation with y: ')\n",
    "print(gdFeature.head())\n",
    "gdFeatureName=gdFeature.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create processed matrix for training and testing\n",
    "X=X[gdFeatureName]\n",
    "testX=testXdf[gdFeatureName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = (['numerical'] *  len(X.columns))\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    sklearn.model_selection.train_test_split(X, y)\n",
    "print('Start!')\n",
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=360, #allow 6 minutes to train the ensemble\n",
    "    per_run_time_limit=30, #allow half minutes to train each model\n",
    "\n",
    ")\n",
    "#this find the ensemble of models \n",
    "automl.fit(X_train, y_train, dataset_name='task1',\n",
    "           feat_type=feature_types)\n",
    "\n",
    "\n",
    "print(automl.show_models())\n",
    "#print score, usually get a better score in publicboard, i d k y\n",
    "predictions = automl.predict(X_test)\n",
    "print(\"R2 score:\", sklearn.metrics.r2_score(y_test, predictions))\n",
    "print('Training Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look up the models inside \n",
    "ensemble=automl.get_models_with_weights()\n",
    "print(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=[]\n",
    "n=5\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y)\n",
    "    #this line does nothing except retraining the model\n",
    "    automl.refit(X_train, y_train)\n",
    "    #print score, usually get a better score in publicboard, i d k y\n",
    "    predictions = automl.predict(X_test)\n",
    "    r2score=sklearn.metrics.r2_score(y_test, predictions)\n",
    "    print(\"R2 score:\", r2score)\n",
    "    r2.append(r2score)\n",
    "print(\"Average R2 score over monte carlo cross validation (n={}):\\t{}\".format(n,np.mean(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain the model and predict the test set\n",
    "automl.refit(X, y)\n",
    "predictions = automl.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warp up the result for the test set\n",
    "result=pd.DataFrame()\n",
    "result['id']=testXdf['id']\n",
    "result['y']= predictions\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save, good luck\n",
    "# result.to_csv('trial1999.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
